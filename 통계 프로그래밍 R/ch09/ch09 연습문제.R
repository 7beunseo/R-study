## ⚠️ 주의
# 1. 잔차, 잔차제곱합, 회귀제곱합 모두 y 값이 기준 
# 2. 회귀식 모형을 잘 보기 (제곱이 있는지, 절편이 없는지, 있는지) 


#####
# 1 #
#####

x <- c(10, 5, 7, 19, 11, 8)
y <- c(15, 9, 3, 25, 7, 13)

# a
lm1 <- lm(y ~ x)
summary(lm1)
# y = -0.6667 + 1.2667 x

# b
lm1$fitted.values
predict(out, newdata=data.frame(x=x)) # x 값에 대한 추정 

# c
lm1$residuals

y - lm1$fitted.values # 관측값(y) - 추정값 (x에 빠는 게 아니라 y에 빼야 함)
lm1$residuals # 잔차 출력 

# d
sum(lm1$residuals ^ 2)
# 101.4667

# e
sum((lm1$fitted.values - mean(y))^2) # mean(y)로 구해야 함. mean(x) 아님! 
# 192.5333

# f
# Adjusted R-squared:  0.5686  -> 설명력 중간 
# Multiple R-squared:  0.6549,	Adjusted R-squared:  0.5686 
# Multiple R-squared -> 얘가 R^2 결정계수임 

# g
plot(y, lm1$residuals)
# 오차의 독립성을 만족함 


#####
# 2 #
#####
# 목화수확량 데이터 (표 9.5)

irrigation <- c(1.8, 1.9, 2.5, 1.4, 1.3, 2.1, 2.3, 1.5, 1.5, 1.2, 1.3, 1.8, 3.5, 3.5)
yield <- c(260, 370, 450, 160, 90, 440, 380, 280, 230, 180, 220, 180, 400, 650)

lm2 <- lm(yield ~ irrigation)
summary(lm2)
# p-value: 0.0001332 -> 일차 회귀계수 유의함 
# y = -24.49 + 167.86 x 
# Multiple R-squared:  0.717,	Adjusted R-squared:  0.6934 

# 각 x점에서 추정값을 구하기
out$fitted.values

# 잔차구하기
out$residuals
yield - out$fitted.values

# 잔차 제곱합 구하기
sum((yield - out$fitted.values)^2)
sum(out$residuals^2)

# 회귀 제곱합 구하기
sum((out$fitted.values - mean(yield))^2) # ⚠️ 제곱해야함 주의 


#####
# 3 #
#####

x1 <- c(10, 5, 7, 19, 11, 18)
x2 <- c(2, 3, 3, 6, 7, 9)
y <- c(15, 9, 3, 25, 7, 13)

# a
### ⚠️ 추정하고자 하는 모형 잘 확인하기, 절편이 없는 상황임 ⚠️ ###
lm3 <- lm(y~0+x1+x2)
summary(lm3)
# hat y = 1.8950 * x1 + -2.0908 * x2

# 검정
# x1 변수에 대한 회귀계수의 유의성 검정 
# H0 : b1 = 0
# H1 : b1 != 0
# t-검정통계량 -> -2.287
# p-value = 0.00909 < 0.05이므로 귀무가설을 기각 -> x1에 대한 회귀계수는 유의함  

# x2 변수에 대한 회귀계수의 유의성 검정 
# H0 : b2 = 0 ## ⚠️ b2 에 대해서 가설을 세워야 함 (b1 아님) 
# H1 : b2 != 0
# t-검정통계량 -> 4.733
# p-value = 0.08414 > 0.05이므로 귀무가설을 기각하지 못함 -> x2에 대한 회귀계수는 유의하지 않음 ## ⚠️ p-value > 0.05 방향 잘 확인하기    

# b
predict(lm3)
predict(lm3, newdata=data.frame(x1=x1, x2=x2))

# c
y - predict(lm3)
lm3$residuals

# d
# 잔차제곱합 
sum(lm3$residuals^2) # 57.8573 

# 회귀제곱합
sum((predict(lm3) -lm3$residuals)^2) # 아님!! 
sum((lm3$fitted.values - mean(y))^2) # 285.8551

# e
# Multiple R-squared:   0.95
# 설명력이 높다.


#####
# 4 #
#####

x <- c(4, 6, 6, 8, 8, 8, 9, 9, 10, 12)
y <- c(9, 10, 18, 20, 15, 17, 20, 22, 25, 30)

# 1
plot(x, y)
lm4 <- lm(y ~ x)
abline(lm4)

# 2
plot(lm4$fitted.values, lm4$residuals)
summary(lm4)
# Multiple R-squared:  0.8497,	Adjusted R-squared:  0.831 -> 설명력이 높다 
# -2.2696 + 2.6087 x
# p-value: 0.0001487 -> 유의수준 0.05에서 귀무가설 기각 -> 추정된 기울기 회귀계수가 유의함 

# 3
length(x)
t_val <- (coef(lm4)[2] - 1.5) / summary(lm4)$coefficients[2,2]
p_val <- 1 - pt(t_val, df = (length(x) - 2))
p_val
# p=value 0.01059865 > 0.01 -> 유의수준 1%에서 귀무가설 기각할 수 없음 -> 기울기는 1.5보다 크다고 할 수 없 (확인 필요) 
# ⚠️ 유의수준 1%이면 0.01과 비교해야 함 


#####
# 5 #
#####

x <- c(40, 50, 60, 70, 80, 90, 40, 60, 80, 50)
y <- c(69, 175, 272, 335, 490, 415, 72, 265, 492, 180)

# a
plot(x, y)

# b
lm5 <- lm(y~x)
summary(lm5)
# y = -252.2971 + 8.5290 x

# c
# p-value: 1.434e-05 < 0.05이므로 유의수준 0.05에서 귀무가설을 기각한다. 추정한 기울기 회구계수가 유의하다.

# d
# Multiple R-squared:  0.9156,	Adjusted R-squared:  0.9051 
# 잘 설명함 

# e
lm5$fitted.values
predict(lm5)

# f
lm5$residuals

# g
plot(lm5$fitted.values, lm5$residuals)
# ?

# h
plot(lm5) # 만족한다고 할 수 있음 
# qqline 그리기 
# 잔차에 대한 qqline이므로 잔차를 대상으로 그려야 함!! 
qqnorm(lm5$residuals)
qqline(lm5$residuals)

# i
predict(lm5, newdata=data.frame(x=mean(x)))
# 276.5 


#####
# 6 #
#####

x <- c(3.5, 4.0, 4.5, 5.0, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0)
y <- c(24.4, 32.1, 37.1, 40.4, 43.3, 43.3, 51.4, 61.9, 66.1, 77.2, 79.2)

# a
plot(x, y)

# b
# 절편이 0
lm6 <- lm(y~x+0)
summary(lm6)
# y = 9.099 x 

# c
# p-value: p-value: 2.29e-11 < 0.05이므로 유의수준 0.05에서 귀무가설을 기각한다. 추정한 기울기 회귀계수가 유의함 

# d 
# Multiple R-squared:  0.9902,	Adjusted R-squared:  0.9892: 0.9892 -> 잘 설명한다.

# b
lm6$fitted.values

# f
lm6$residuals

# g
plot(lm6$fitted.values, lm6$residuals)

# h
qqnorm(lm6$residuals)
qqline(lm6$residuals)

# i
predict(lm6, newdata=data.frame(x=5.3))
# 48.22401 


#####
# 7 #
#####

x <- c(280, 250, 300, 320, 310, 280, 320, 300, 320)
y <- c(2.1, 3.0, 3.2, 1.4, 2.6, 2.7, 1.3, 3.4, 2.8)

# a
plot(x, y)
abline(lm7)

# b
lm7 <- lm(y~x)
summary(lm7)
# y = 6.42195 + -0.01317 x

# c
# t-통계량 = -1.221
# p-value: 0.2616 > 0.05이므로 유의수준 0.05에서 귀무가설을 기각할 수 없다. 추정된 기울기 회귀계수가 유의하지 않다. 

# d
# Multiple R-squared:  0.1756,	Adjusted R-squared:  0.05784 
# 거의 설명하지 못함 

# e
lm7$fitted.values

# f
lm7$residuals

# g
plot(lm7$fitted.values, lm7$residuals)

# h
plot(lm7)

# i
predict(lm7, newdata=data.frame(x=400))
# 1.153659


#####
# 8 #
#####

data(cars)
attach(cars)
plot(speed, dist)
abline(dist_1)

# a
dist_1 <- lm(dist~speed)
summary(dist_1)
# H0: b1 = 0, H1 : b1 != 0
# p-value: 1.49e-12 < 0.05이므로 귀무가설을 기각한다. 추정한 기울기 회귀계수 유의하다
# Multiple R-squared:  0.6511  -> 중간정도 설명 
# y = -17.5791 + 3.9324 x 

# b
dist_2 <- lm(dist~speed + I(speed^2))
summary(dist_2)
# H0: b1 = 0, H1 : b1 != 0
# p-value: 0.656 > 0.05이므로 유의수준 0.05에서 귀무가설을 기각할 수 없다. 추정한 일차 회귀계수 유의하지 않음 
# H0 : b2 = 0, H1 : b2 != 0
# p-value: 0.136 > 0.05이므로 유의수준 0.05에서 귀무가설을 기각할 수 없다. 추정한 이차 회귀계수 유의하지 않음 
# Multiple R-squared:  0.6673  중간정도 설명 
# y = 2.47014 + 0.91329 x + 0.09996 + x^2

# c
dist_3 <- lm(dist~speed + 0)
summary(dist_3)
# H0: b1 = 0, H1 : b1 != 0
# p-value: < 2.2e-16 < 0.05이므로 유의수준 0.05에서 귀무가설을 기각한다. 추정한 기울기 회귀계수 유의
# y = 2.9091 x
# Multiple R-squared:  0.8963  -> 잘 설명함 

#####
# 9 #
#####

Y <- c(2.04, 2.56, 3.75, 1.10, 3.00, 0.05, 1.38, 1.50, 1.38, 4.01, 1.50, 1.29, 1.90, 3.11, 1.92, 0.81, 
       1.90, 3.66, 2.00, 2.05, 2.60, 1.56, 0.38, 2.48, 2.74, 1.77, 2.12, 0.99, 1.62, 2.03, 3.50, 3.18,
       2.39, 1.48, 2.51, 1.57, 2.46, 2.42, 2.17, 2.04)

X1 <- c(2.01, 3.40, 3.68, 1.54, 3.32, 0.33, 0.36, 1.97, 2.03, 2.05, 2.13, 1.34, 1.51, 3.12, 2.14, 2.60,
        1.90, 3.06, 1.60, 1.96, 1.96, 1.50, 1.60, 1.92, 3.09, 0.78, 2.12, 1.85, 1.78, 1.03, 3.44, 2.42,
        1.74, 1.89, 1.43, 1.64, 2.69, 1.79, 2.13, 2.15)

X2 <- c(1070, 1254, 1466, 706, 1106, 756, 1058, 1008, 1104, 1200, 896, 848, 958, 1246, 1106, 790,
        954, 1500, 1046, 1054, 1198, 940, 456, 1150, 636, 744, 644, 842, 852, 1170, 1034, 1202,
        1018, 1180, 952, 1038, 1090, 694, 1096, 1114)

X3 <- c(5, 6, 6, 4, 5, 3, 2, 7, 4, 7, 7, 3, 5, 6, 4, 5, 
        4, 6, 5, 4, 6, 3, 6, 7, 6, 5, 5, 3, 5, 3, 10, 5,
        5, 5, 3, 4, 6, 5, 6, 6)

# a
plot(X1, Y)
plot(X2, Y)
plot(X3, Y)

# b
lm9 <- lm(Y~X1+X2+X3)
summary(lm9)
# hat Y = -1.1547620 + 0.3650335 * X1 + 0.0018275 * X2 + 0.1293062 * X3

# c
# p-value =  0.033539 < 0.05이므로 유의수준 0.05에서 귀무가설을 기각한다.-> X1 회귀계수는 유의하다

# d
# p-value = 0.000332 < 0.05이므로 유의수준 0.05에서 귀무가설을 기각한다. -> X2 회귀계수는 유의하다

# e
# p-value = 0.104760 > 0.05이므로 귀무가설을 기각할 수 없다. -> X3 회귀계수는 유의하지 않다.

# f 
# Multiple R-squared:  0.6145
# 설명력이 중간이다. 


######
# 10 #
######

run100 <- c(11.43, 11.41, 11.31, 11.00, 11.95, 11.42, 11.15, 11.79, 11.45, 11.95,
            11.29, 11.73, 11.73, 11.96, 12.23, 11.89, 11.76, 11.13, 11.81, 11.44,
            12.30, 11.80, 11.16, 10.79)

run200 <- c(23.09, 23.04, 23.17, 22.25, 24.41, 23.52, 22.59, 24.08, 23.06, 24.28,
            23.00, 24.00, 23.88, 24.49, 24.21, 23.62, 23.54, 22.21, 24.22, 23.46,
            25.00, 23.98, 22.82, 21.83)

run400 <- c(50.62, 52.00, 52.80, 50.06, 54.97, 53.60, 51.73, 54.93, 51.50, 53.60,
            52.01, 53.73, 52.70, 55.70, 55.09, 53.76, 54.60, 49.29, 54.30, 51.20,
            55.08, 53.59, 51.79, 50.62)

run800 <- c(1.99, 2.00, 2.10, 2.00, 2.08, 2.01, 2.00, 2.07, 2.01, 2.10,
            1.96, 2.09, 2.00, 2.15, 2.19, 2.04, 2.19, 1.95, 2.09, 1.92,
            2.12, 2.05, 2.02, 1.96)

run1500 <- c(4.22, 4.14, 4.49, 4.06, 4.33, 4.21, 4.14, 4.35, 4.14, 4.32,
             3.98, 4.35, 4.15, 4.42, 4.69, 4.25, 4.60, 3.99, 4.16, 3.96,
             4.52, 4.14, 4.12, 3.95)

run3000 <- c(9.34, 8.88, 9.77, 8.81, 9.31, 8.71, 8.98, 9.87, 8.98, 9.12,
             8.63, 9.20, 9.20, 9.62, 10.46, 9.59, 10.16, 8.97, 8.84, 8.53,
             9.42, 9.02, 8.84, 8.50)

marathon <- c(159.37, 157.85, 168.75, 149.45, 168.48, 151.75, 155.27, 182.20, 156.47, 188.03,
              151.82, 150.50, 181.05, 164.65, 182.17, 158.53, 200.37, 160.82, 151.20, 165.45,
              182.77, 162.60, 154.48, 142.72)

data <- cbind(run100, run200, run400, run800, run1500, run3000, marathon)

# a
apply(data, 2, mean)
apply(data, 2, var)
apply(data, 2, sd)
apply(data, 2, median)
apply(data, 2, max)
apply(data, 2, min)

# b
cor(run100, run200)

# c
cor(run100, marathon)

# d
cor(run3000, marathon)


######
# 11 #
######

firm <- 1:20

salary <- c(3030, 6050, 3571, 3300, 0, 9375, 9525, 6000, 999, 3300,
            3500, 2493, 1911, 2130, 1185, 5236, 1990, 6000, 6229, 1523)

tenure <- c(7, 0, 11, 6, 18, 6, 15, 5, 3, 2,
            16, 5, 7, 4, 0, 2, 4, 32, 5, 3)

age <- c(61, 51, 63, 60, 63, 57, 60, 61, 57, 60,
         63, 61, 58, 59, 56, 60, 60, 74, 63, 56)

sales <- c(161315, 144416, 139208, 100697, 100469, 81667, 76431, 57813, 56154, 53588,
           50777, 47678, 47061, 41322, 37154, 35853, 33674, 33296, 32379, 31707)

profits <- c(2956, 22071, 4430, 6370, 9296, 6328, 5807, 5372, 1120, 6398,
             5165, 1704, 2945, 1048, 3780, 1259, 568, 3765, 3782, 578)

assets <- c(257389, 237545, 49271, 92630, 355935, 86100, 668641, 59920, 36672, 59550,
            617679, 42754, 33673, 37675, 30966, 299804, 14166, 194398, 365875, 28570)

# a
lm11 <- lm(salary~profits)
summary(lm11)           
# hat salary = 3066.9981 + 0.1690 * profit
# p-value = 0.19742 -> 귀무가설 기각 x, 일차 회귀계수가 유의하지 않다.
plot(profits, salary)
abline(lm11)

# b
lm11_b <- lm(salary~profits+age)
summary(lm11_b)

# c
lm11_c <- lm(salary~profits+age+sales)
summary(lm11_c)
# 회귀계수 모두 유의하지 않음 

# d
lm11_d <- lm(salary~profits+age+sales+tenure)
summary(lm11_d)

# e
lm11_e <- lm(salary~profits+age+sales+tenure+assets)
summary(lm11_e)

